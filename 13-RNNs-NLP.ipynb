{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZVJ8gUn1RR9"
      },
      "source": [
        "# Módulo 5: Introducción a las RNN y el PLN en PyTorch\n",
        "\n",
        "Hasta ahora, no hemos aplicado las capacidades de PyTorch para resolver problemas de procesamiendo del lenguaje natural (PLN). El campo del PLN es un ámbito muy emocionante en el que aplicar las técnicas de *deep learning*. Al igual que sucede con la visión artificial, existen diferentes tipos de problemas de PLN. Entre ellos, la clasificación, el resumen y el mapeado secuencia a secuencia. Por último, queremos implementar un modelo que resuelva un problema muy habitual de mapeado secuencia a secuencia: la traducción automática. Sin embargo, para desarrollar las herramientas de PLN necesarias, antes examinaremos tareas más sencillas, como completar tareas y frases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptJiFULG2AiJ"
      },
      "source": [
        "## Definición del problema\n",
        "\n",
        "Los problemas de mapeado secuencia a secuencia son, por su propia naturaleza, diferentes de las tareas de aprendizaje que hemos resuelto hasta ahora. En la clasificación de imágenes, existe una etiqueta de clase diferenciada, una correspondencia uno a uno entre el *input* y el *output*. En el mapeado secuencia a secuencia, el *input* consiste en muchas muestras discretas o, en este caso, caracteres. Además, el *output* es también una secuencia de caracteres."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu4mucQioGZA"
      },
      "source": [
        "## Conjunto de datos\n",
        "\n",
        "Utilizaremos una base de datos de pares de frases en inglés y español. Más adelante, utilizaremos el mismo conjunto para una tarea de traducción automática, pero de momento solo nos interesa examinar las frases en inglés. \n",
        "\n",
        "El tutorial de PyTorch [NLP From Scratch](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) utiliza un conjunto de datos en inglés y francés para realizar una tarea de traducción, y fue lo que nos inspiró a utilizar este conjunto. Realizaremos un proceso de limpieza de datos y aplicaremos funciones de utilidad similares a las del tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yrzZRul42zJs"
      },
      "outputs": [],
      "source": [
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import random\n",
        "import re\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import Subset\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import time, copy\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoRgk40z3kJk",
        "outputId": "0336ced6-ef28-4362-c034-4e2892ddb8ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'spa-eng.zip': No such file or directory\n",
            "rm: cannot remove '_about.txt': No such file or directory\n",
            "rm: cannot remove 'spa.txt': No such file or directory\n",
            "--2023-01-17 19:54:58--  https://www.manythings.org/anki/spa-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5336731 (5,1M) [application/zip]\n",
            "Saving to: ‘spa-eng.zip’\n",
            "\n",
            "spa-eng.zip         100%[===================>]   5,09M  1,60MB/s    in 3,2s    \n",
            "\n",
            "2023-01-17 19:55:04 (1,60 MB/s) - ‘spa-eng.zip’ saved [5336731/5336731]\n",
            "\n",
            "Archive:  spa-eng.zip\n",
            "  inflating: _about.txt              \n",
            "  inflating: spa.txt                 \n",
            "10-CNNs.ipynb\t\t\t  7-pytorch-ejemplo.ipynb\n",
            "11-TAREA-FashionMNIST.ipynb\t  8-TAREA-MNIST.ipynb\n",
            "12-Transfer_Learning.ipynb\t  9-Autoencoders.ipynb\n",
            "13-RNNs-NLP.ipynb\t\t  abalone.data\n",
            "1-grad_desc.ipynb\t\t  abalone.names\n",
            "2-gradiente.ipynb\t\t  _about.txt\n",
            "3-python-classes.ipynb\t\t  FashionMNIST\n",
            "4-neurona.ipynb\t\t\t  gtsrb\n",
            "5-TAREA-Vinos.ipynb\t\t  MNIST\n",
            "6.5.0-TEST.ipynb\t\t  spa-eng.zip\n",
            "6.5.1-TEST.ipynb\t\t  spa.txt\n",
            "6.5.2-TEST.ipynb\t\t  winequality-red.csv\n",
            "6.5.3-TEST.ipynb\t\t  winequality-white.csv\n",
            "6-redes-neuronales-pytorch.ipynb\n"
          ]
        }
      ],
      "source": [
        "# Descargar y descomprimir el archivo de texto que contiene todas las frases traducidas\n",
        "!rm spa-eng.zip _about.txt spa.txt\n",
        "!wget https://www.manythings.org/anki/spa-eng.zip\n",
        "!unzip spa-eng.zip\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KUUMOgBsqIq"
      },
      "source": [
        "Estas funciones de ayuda limpian el conjunto de datos para garantizar que la red neuronal pueda utilizar las frases. Para obtener más información, visite el enlace que aparece en el siguiente comentario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "66lR-Qyl8BYs"
      },
      "outputs": [],
      "source": [
        "# Funciones de ayuda combinadas del tutorial de PyTorch: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "\n",
        "# Convertir una string de Unicode a ASCII plano, gracias a\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Convertir los caracteres a minúsculas, recortar y eliminar todos los caracteres que no sean letras\n",
        "# Este paso es importante para que todas las palabras tengan el mismo formato,\n",
        "# igual que sucede al normalizar imágenes\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\"\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!'?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "def parse_data(filename):\n",
        "    # Leer el archivo y dividir en líneas\n",
        "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "\n",
        "    # Dividir cada línea en pares y normalizar\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "    # Eliminar la atribución, porque no forma parte de los datos\n",
        "    pairs = [[pair[0], pair[1]] for pair in pairs]\n",
        "\n",
        "    return pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ti0WFHKX9-k-",
        "outputId": "0595117b-ebf7-44b6-912a-02ba459e746c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of English sentences: 139013\n"
          ]
        }
      ],
      "source": [
        "pairs = parse_data(\"spa.txt\")\n",
        "# Solo nos interesan las frases en inglés, porque no vamos a traducir\n",
        "english_sentences = [pair[0] for pair in pairs]\n",
        "# Mezclar el conjunto de datos\n",
        "random.shuffle(english_sentences)\n",
        "print(\"Number of English sentences:\", len(english_sentences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IxR_MgYJ3-dt"
      },
      "outputs": [],
      "source": [
        "# Incluiremos todas las letras ascii y algunos caracteres habituales en el conjunto de caracteres válidos\n",
        "letters = string.ascii_letters + \" .,;'-\"\n",
        "num_letters = len(letters) + 1 # Tenemos que añadir 1 para tener en cuenta la etiqueta \"Fin\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvh6cZbVd_Ro",
        "outputId": "a54e5c2f-279d-418d-8a4d-7c3c6d9d1995"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\n"
          ]
        }
      ],
      "source": [
        "print(string.ascii_letters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxGSbaJKzoHj"
      },
      "source": [
        "## Completar palabras\n",
        "\n",
        "Para la primera ronda, vamos a tomar una muestra de las palabras que aparecen en las frases y a predecir palabras completas a partir de un subconjunto de caracteres."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BngUGRRE-8P0"
      },
      "outputs": [],
      "source": [
        "# Reducir las frases a palabras y eliminar '.'\n",
        "words = [word for sentence in english_sentences for word in sentence.split(' ') if word not in ['.', '!', '?'] and len(word) > 0 and '\\'' not in word]\n",
        "\n",
        "# Como ya hemos mezclado el conjunto de datos, tomamos una muestra aleatoria de palabras para los subconjuntos de entrenamiento, validación y prueba\n",
        "train_words = words[:20000]\n",
        "val_words = words[20000:25000]\n",
        "test_words = words[25000:30000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lRu7wFReUMY",
        "outputId": "0716eb66-be91-4cba-9ca5-21f7a6425891"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['have', 'a', 'nice', 'weekend', 'we', 'came', 'together', 'just', 'let', 'me', 'sleep', 'i', 'have', 'no', 'time', 'for', 'games', 'stop', 'asking', 'so', 'many', 'questions', 'tom', 'told', 'me', 'that', 'he', 'spoke', 'french', 'he', 'had', 'a', 'stroke', 'tom', 'went', 'out', 'to', 'eat', 'why', 'are', 'you', 'two', 'always', 'fighting', 'tom', 'stood', 'up', 'again', 'i', 'heard', 'you', 'were', 'back', 'in', 'town', 'tom', 'fed', 'his', 'dog', 'table', 'scraps', 'tom', 'sprained', 'his', 'ankle', 'terrible', 'advice', 'koko', 'is', 'a', 'female', 'gorilla', 'old', 'soldiers', 'never', 'die', 'they', 'just', 'fade', 'away', 'this', 'will', 'keep', 'you', 'warm', 'i', 'want', 'you', 'to', 'understand', 'my', 'situation', 'could', 'you', 'please', 'explain', 'it', 'once', 'more', 'i']\n"
          ]
        }
      ],
      "source": [
        "print(train_words[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pszv87RStebQ"
      },
      "source": [
        "### Codificación\n",
        "\n",
        "Ahora que tenemos todas las palabras en formato de *string*, debemos codificarlas en una representación que la red neuronal pueda interpretar. Crearemos un vector de codificación *one-hot* para cada caracter, y los encadenaremos para que representen una palabra específica para las muestras de *input* de entrenamiento.\n",
        "\n",
        "Puesto que estamos utilizando una RNN, introduciremos estos vectores de caracteres de uno en uno. El objetivo de la RNN es predecir el *siguiente* caracter de la palabra. Eso permite a la red generar una palabra a partir de una única letra o un prefijo. Además, necesitamos que la red aprenda cuándo termina una palabra. Para ello, designaremos un caracter especial al final de cada secuencia (EOS) que se debería predecir a partir del último caracter de una palabra.\n",
        "\n",
        "Designaremos dos secuencias de tensores. En primer lugar, el `input_tensor`, que se representa como una concatenación de los vectores de codificación *one-hot* para cada caracter. La segunda secuencia de tensores, `target_tensor`, contiene el vector de codificación *one-hot* para los caracteres de la secuencia de *output*.\n",
        "\n",
        "Por lo tanto, la secuencia objetivo será la codificación de `input_word[1:]` más un símbolo adicional que represente el final de la palabra.Esto alinea cada uno de los vectores de codificación *one-hot* de la muestra de *input* de entrenamiento con el vector de codificación *one-hot* que está intentando predecir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3MLDsB9KSYvQ"
      },
      "outputs": [],
      "source": [
        "# Crear las muestras de entrenamiento:\n",
        "\n",
        "# Adaptado de https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html\n",
        "# Matriz one-hot de letras de la primera a la última (sin incluir EOS) como input\n",
        "def input_tensor(word):\n",
        "    tensor = torch.zeros(len(word), 1, num_letters)\n",
        "    for idx in range(len(word)):\n",
        "        letter = word[idx]\n",
        "        tensor[idx][0][letters.find(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "# LongTensor desde la segunda letra al final (EOS) como objetivo\n",
        "def target_tensor(word):\n",
        "    tensor = torch.zeros(len(word), 1, num_letters)\n",
        "    for idx in range(1, len(word)):\n",
        "        letter = word[idx]\n",
        "        tensor[idx-1][0][letters.find(letter)] = 1\n",
        "    tensor[len(word)-1][0][num_letters-1] = 1 # EOS\n",
        "    return tensor\n",
        "\n",
        "train_input = [input_tensor(word) for word in train_words]\n",
        "train_target = [target_tensor(word) for word in train_words]\n",
        "val_input = [input_tensor(word) for word in val_words]\n",
        "val_target = [target_tensor(word) for word in val_words]\n",
        "test_input = [input_tensor(word) for word in test_words]\n",
        "test_target = [target_tensor(word) for word in test_words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqsnvhHEaWCF",
        "outputId": "034443c1-35e9-46b3-d3b4-9fb3378245c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "have [encode as] tensor([[[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
            "a [encode as] tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
            "nice [encode as] tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
            "weekend [encode as] tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
            "we [encode as] tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
            "came [encode as] tensor([[[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]]])\n"
          ]
        }
      ],
      "source": [
        "# Veamos el aspecto que tienen las codificaciones de las palabras\n",
        "for i in range(6):\n",
        "    print(train_words[i], \"[encode as]\", train_input[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrcaK-ySaR6d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnIg_qVDQhy0",
        "outputId": "6354d752-3c1b-4d91-f453-3024ab691d5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This code helps visualize what an input and its corresponding target should look like!\n",
            "have\n",
            "ave<EOS>\n"
          ]
        }
      ],
      "source": [
        "def input_tensor_to_word(tensor):\n",
        "    word = \"\"\n",
        "    for i in range(tensor.size(0)):\n",
        "        topv, topi = tensor[i].topk(1)\n",
        "        word += letters[topi[0][0]]\n",
        "    return word\n",
        "\n",
        "def target_tensor_to_word(tensor):\n",
        "    word = \"\"\n",
        "    for i in range(tensor.size(0)):\n",
        "        topv, topi = tensor[i].topk(1)\n",
        "        if topi[0][0] == num_letters-1:\n",
        "            word += \"<EOS>\"\n",
        "            break\n",
        "        word += letters[topi[0][0]]\n",
        "    return word\n",
        "print(\"This code helps visualize what an input and its corresponding target should look like!\")\n",
        "for input, target in zip(train_input, train_target):\n",
        "    print(input_tensor_to_word(input))\n",
        "    print(target_tensor_to_word(target))\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SjmOheE6bcn",
        "outputId": "11ff53a4-4859-41c2-8d8a-d17d120770d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset_sizes = {'train': 20000, 'val': 5000, 'test': 5000}\n"
          ]
        }
      ],
      "source": [
        "dataloaders = {'train': list(zip(train_input, train_target)),\n",
        "               'val': list(zip(val_input, val_target)),\n",
        "               'test': list(zip(test_input, test_target))}\n",
        "\n",
        "dataset_sizes = {'train': len(train_input),\n",
        "                 'val': len(val_input),\n",
        "                 'test': len(test_input)}\n",
        "print(f'dataset_sizes = {dataset_sizes}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKtltT2-oUuo"
      },
      "source": [
        "### Definición de la RNN\n",
        "\n",
        "Para completar esta actividad, crearemos una implementación básica de una red neuronal recurrente (RNN). Afortunadamente, PyTorch incluye una muy buena implementación de un módulo RNN. Lo utilizaremos y modificaremos nuestro método `forward` estándar para que tenga en cuenta el estado oculto.\n",
        "\n",
        "Al utilizar una RNN, la red neuronal es capaz de generar predicciones de secuencias de caracteres. Se modificará ligeramente el *feedforward* para incluir un estado oculto que codifique información sobre la secuencia actual que la red neuronal ya ha visto. Además del estado oculto codificado, el *output* del *feedforward* será la probabilidad predicha del proximo carácter de la secuencia.\n",
        "\n",
        "El entrenamiento será ligeramente diferentes, ya que la red predecirá probabilidades sobre el próximo carácter de la secuencia. Por lo tanto, haremos un seguimiento de la pérdida para toda una secunecia antes de realizar la retropropagación. En la siguiente sección, lo explicaremos con más detalle.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "bYLj4Qvzxldp"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers = 1, nonlinearity = 'tanh', dropout = 0)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "\n",
        "        output, hidden = self.rnn(input, hidden)\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output, hidden\n",
        "    \n",
        "    # Este método nos dará un tensor de estado oculto para la predicción inicial de la secuencia\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DoQcZxUF-QBY"
      },
      "outputs": [],
      "source": [
        "# Para esta red, utilizaremos un tamaño oculto de 128\n",
        "word_rnn = RNN(num_letters, 128, num_letters).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keo1PNzIk8Y8",
        "outputId": "0dc438f7-b7b3-4a28-ee14-bf9d24638903"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RNN(\n",
            "  (rnn): RNN(59, 128)\n",
            "  (fc): Linear(in_features=128, out_features=59, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(word_rnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUr2jUTXxl6Q"
      },
      "source": [
        "### Entrenamiento\n",
        "\n",
        "Entrenar el modelo será un proceso muy similar a las tareas de clasificación que hemos completado en otras ocasiones. Sin embargo, ahora tendremos que tener en cuenta la secuencia de *inputs* y predicciones. Al entrenar, calcularemos la pérdida de toda una secuencia (palabra) y aplicaremos la retropropagación de cada secuencia dada.\n",
        "\n",
        "Podemos definir así los pasos necesarios:\n",
        "\n",
        "1. Inicializar el estado oculto - Cada llamada de `forward` requiere una codificación de estado oculto. Hemos decidido que nuestra codificación inicial de estado oculto para el primer caracter de una secuencia es un tensor `zeros`.\n",
        "\n",
        "2. Completar ciclos del `input_tensor` y aplicar los siguientes pasos a cada carácter de la secuencia:\n",
        "\n",
        "    a. Llamar a `forward` con la codificación del carácter actual y el `hidden_state`actual.\n",
        "\n",
        "    b. `forward` generará como *output* una distribución de probabilidad del próximos carácter predicho y un nuevo `hidden_state`\n",
        "\n",
        "    c. Calcular la pérdida de la distribución del *output* de probabilidad del siguiente carácter respecto al siguiente carácter *ground truth* de la secuencia y sumarla al total en curso.\n",
        "\n",
        "    d. Pasar al siguiente carácter de la secuencia de *input* utilizando el último `hidden_state`\n",
        "    \n",
        "3. Una vez que todos los caracteres de la secuencia hayan pasado por la función `forward`, realizar la retropropagación dada la pérdida entera de la secuencia y rehacer este proceso para la siguiente secuencia.\n",
        "\n",
        "Nota: se ha actualizado la función de entrenamiento para representar este nuevo flujo de trabajo. Intente seguir las isntrucciones que aparecen en los comentarios del código para ver cómo hemos implementado este nuevo proceos en Python.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRR_nA2ofl8t"
      },
      "outputs": [],
      "source": [
        "def train_rnn(model, dataloaders, dataset_sizes, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict()) # Almacenar los mejores pesos por separado\n",
        "    best_loss = np.inf\n",
        "    best_epoch = 0\n",
        "\n",
        "    # Cada ciclo tiene una fase de entrenamiento, validación y prueba\n",
        "    phases = ['train', 'val', 'test']\n",
        "    \n",
        "    # Hacer un seguimiento de la evolución de la pérdida durante el entrenamiento\n",
        "    training_curves = {}\n",
        "    for phase in phases:\n",
        "        training_curves[phase+'_loss'] = []\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in phases:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Configurar el modelo en modo de entrenamiento\n",
        "            else:\n",
        "                model.eval()   # Configurar el modelo en modo de evaluación\n",
        "                \n",
        "            running_loss = 0.0\n",
        "\n",
        "           # Iterar con los datos\n",
        "            for input_sequence, target_sequence in dataloaders[phase]:\n",
        "                # Ahora, iterar con cada secuencia\n",
        "\n",
        "                hidden = model.initHidden() # Empezar con un estado oculto nuevo\n",
        "\n",
        "                current_input_sequence = input_sequence.to(device)\n",
        "                current_target_sequence = target_sequence.to(device)\n",
        "\n",
        "                # Poner a cero los gradientes de los parámetros\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Método forward\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    loss = 0\n",
        "                    # Hacer una predicción de cada elemento de la secuencia,\n",
        "                    # y llevar un seguimiento del estado oculto a lo largo del proceso\n",
        "                    for i in range(current_input_sequence.size(0)):\n",
        "                        # Debemos pensar bien cómo transferir las capas ocultas al dispositivo\n",
        "                        current_hidden = hidden.to(device)\n",
        "                        output, hidden = model(current_input_sequence[i], current_hidden)\n",
        "                        l = criterion(output, current_target_sequence[i])\n",
        "                        loss += l\n",
        "\n",
        "                    # Método backward y actualización de los pesos solo si está en la fase de entrenamiento al final de una secuencia\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # Estadísticas\n",
        "                running_loss += loss.item() / current_input_sequence.size(0)\n",
        " \n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            training_curves[phase+'_loss'].append(epoch_loss)\n",
        "\n",
        "            print(f'{phase:5} Loss: {epoch_loss:.4f}')\n",
        "\n",
        "            # Hacer una copia profunda del modelo si es la mejor pérdida\n",
        "            # Nota: utilizamos la pérdida de entrenamiento para seleccionar el mejor modelo\n",
        "            if phase == 'train' and epoch_loss < best_loss:\n",
        "              best_epoch = epoch\n",
        "              best_loss = epoch_loss\n",
        "              best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Loss: {best_loss:4f} at epoch {best_epoch}')\n",
        "\n",
        "    # Cargar los mejores pesos del modelo\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    \n",
        "    return model, training_curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIGWf9aAFpBD"
      },
      "outputs": [],
      "source": [
        "# Debería tener un aspecto muy similar al de tareas anteriores\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10 # Solo vamos a aplicar 10 ciclos para ahorrar tiempo, pero puede utilizar más si quiere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzqzmUwL8Chj",
        "outputId": "9e7a8cfa-3e3c-42c0-cf5b-c29717ff8640"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/10\n",
            "----------\n",
            "train Loss: 1.6566\n",
            "val   Loss: 1.4036\n",
            "test  Loss: 1.4166\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "train Loss: 1.3388\n",
            "val   Loss: 1.2824\n",
            "test  Loss: 1.2946\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "train Loss: 1.2420\n",
            "val   Loss: 1.2191\n",
            "test  Loss: 1.2260\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "train Loss: 1.1889\n",
            "val   Loss: 1.1915\n",
            "test  Loss: 1.1973\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "train Loss: 1.1566\n",
            "val   Loss: 1.1703\n",
            "test  Loss: 1.1749\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "train Loss: 1.1315\n",
            "val   Loss: 1.1472\n",
            "test  Loss: 1.1509\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "train Loss: 1.1124\n",
            "val   Loss: 1.1360\n",
            "test  Loss: 1.1387\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "train Loss: 1.0966\n",
            "val   Loss: 1.1283\n",
            "test  Loss: 1.1296\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "train Loss: 1.0838\n",
            "val   Loss: 1.1217\n",
            "test  Loss: 1.1228\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "train Loss: 1.0714\n",
            "val   Loss: 1.1124\n",
            "test  Loss: 1.1120\n",
            "\n",
            "Training complete in 14m 3s\n",
            "Best val Loss: 1.071431 at epoch 9\n"
          ]
        }
      ],
      "source": [
        "# El código es muy similar al de modelos de entrenamiento anteriores no recurrentes\n",
        "# Pérdida y optimizador\n",
        "criterion = nn.CrossEntropyLoss() # Utilizar CrossEntropyLoss para la clasificación.\n",
        "optimizer = torch.optim.Adam(word_rnn.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "\n",
        "# Entrenar el modelo. También almacenaremos los resultados del entrenamiento para poder visualizarlos\n",
        "word_rnn, training_curves = train_rnn(word_rnn, dataloaders, dataset_sizes, \n",
        "                                     criterion, optimizer, scheduler, num_epochs=num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Diwgt3Fix2zg"
      },
      "source": [
        "### Visualizar los resultados\n",
        "\n",
        "Podemos visualizar la curva de entrenamiento, igual que en las tareas anteriores. Parece que, después de 10 ciclos, el modelo no se sobreajusta, lo que significa que probablemente podríamos entrenarlo durante más tiempo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEWXj81iCrvE"
      },
      "outputs": [],
      "source": [
        "def plot_training_curves(training_curves, \n",
        "                         phases=['train', 'val', 'test'],\n",
        "                         metrics=['loss']):\n",
        "    epochs = list(range(len(training_curves['train_loss'])))\n",
        "    for metric in metrics:\n",
        "        plt.figure()\n",
        "        plt.title(f'Training curves - {metric}')\n",
        "        for phase in phases:\n",
        "            key = phase+'_'+metric\n",
        "            if key in training_curves:\n",
        "                plt.plot(epochs, training_curves[key])\n",
        "        plt.xlabel('epoch')\n",
        "        plt.legend(labels=phases)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "BgaR32cNCxUC",
        "outputId": "03f10ee0-4884-4023-d894-60896ecada78"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_training_curves(training_curves, phases=['train', 'val', 'test'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjpv3rrAyUST"
      },
      "source": [
        "### Generar palabras\n",
        "\n",
        "Más abajo, hemos incluido una función que ejecuta la red sobre una secuencia de *input* especificada, que podría ser una letra o parte de una palabra. Encontrará también algunos ejemplos, puede utilizarlos para conocer la palabra que predice la red.\n",
        "\n",
        "Los resultados de esta red dependerán, en gran medida, de la muestra aleatoria de frases que se le dieran al principio. Cuanto más aparezca una palabra concreta en los datos de entrenamiento, más probable es que aparezca en la predicción. Funciona de manera muy similar a las sugerencias de palabras del teclado de los *smartphones*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h35LR8siENSg"
      },
      "outputs": [],
      "source": [
        "def predict(model, input_letters, max_length = 50):\n",
        "    output_word = input_letters\n",
        "    tensor = input_tensor(input_letters)\n",
        "    hidden = model.initHidden()\n",
        "    current_input_sequence = tensor.to(device)\n",
        "    input = None\n",
        "\n",
        "    for i in range(current_input_sequence.size(0)):\n",
        "        current_hidden = hidden.to(device)\n",
        "        output, hidden = model(current_input_sequence[i], current_hidden)\n",
        "\n",
        "    topv, topi = output.topk(1)\n",
        "    topi = topi[0][0]\n",
        "    if topi == num_letters - 1:\n",
        "        # Imprimir(\"Most likely word was our initial letters, grab the second most likely\")\n",
        "        topv, topi = output.topk(2)\n",
        "        topi = topi[0][1]\n",
        "    letter = letters[topi]\n",
        "    output_word += letter\n",
        "    input = input_tensor(letter)\n",
        "\n",
        "    for i in range(len(input_letters), max_length):\n",
        "        current_hidden = hidden.to(device)\n",
        "        current_input = input[0].to(device)\n",
        "        output, hidden = model(current_input, current_hidden)\n",
        "        topv, topi = output.topk(1)\n",
        "        topi = topi[0][0]\n",
        "        if topi == num_letters - 1:\n",
        "            # Imprimir(\"Hit the EOS\")\n",
        "            break\n",
        "        letter = letters[topi]\n",
        "        output_word += letter\n",
        "        input = input_tensor(letter)\n",
        "    return output_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbYyrIeVnLsE",
        "outputId": "64d99bc9-1540-4d63-f48e-1c65bfd120ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "have\n",
            "along\n",
            "could\n"
          ]
        }
      ],
      "source": [
        "print(predict(word_rnn, \"ha\"))\n",
        "print(predict(word_rnn, \"al\"))\n",
        "print(predict(word_rnn, \"c\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTTaTWhPLMTN"
      },
      "source": [
        "\n",
        "## Completar frases\n",
        "\n",
        "Ahora, nos centraremos en la tarea de completar frases. Es un proceso relativamente sencillo, porque seguiremos trabajando con codificaciones *one-hot* de secuencias de caracteres. Existen otras formas más sofisticadas de codificar las muestras de entrenamiento. Las veremos más adelante."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFSKAkfB3Mm_"
      },
      "outputs": [],
      "source": [
        "train_sentences = english_sentences[:1000]\n",
        "val_sentences = english_sentences[1000:2000]\n",
        "test_sentences = english_sentences[2000:3000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRM0QT3Q0Wvt"
      },
      "source": [
        "### Codificación\n",
        "\n",
        "Esta codificación es muy sencilla. Usaremos la misma función que antes, pero ahora para operar en toda la frase en vez de en una palabra."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRNqiQSw3K79"
      },
      "outputs": [],
      "source": [
        "train_input_sentence = [input_tensor(sentence) for sentence in train_sentences]\n",
        "train_target_sentence = [target_tensor(sentence) for sentence in train_sentences]\n",
        "val_input_sentence = [input_tensor(sentence) for sentence in val_sentences]\n",
        "val_target_sentence = [target_tensor(sentence) for sentence in val_sentences]\n",
        "test_input_sentence = [input_tensor(sentence) for sentence in test_sentences]\n",
        "test_target_sentence = [target_tensor(sentence) for sentence in test_sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaJtJZJL4KYF",
        "outputId": "e6145c4f-144d-42ad-a3b7-6d6677c534ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset_sizes = {'train': 1000, 'val': 1000, 'test': 1000}\n"
          ]
        }
      ],
      "source": [
        "dataloaders_sentences = {'train': list(zip(train_input_sentence, train_target_sentence)),\n",
        "               'val': list(zip(val_input_sentence, val_target_sentence)),\n",
        "               'test': list(zip(test_input_sentence, test_target_sentence))}\n",
        "\n",
        "dataset_sizes_sentences = {'train': len(train_input_sentence),\n",
        "                 'val': len(train_input_sentence),\n",
        "                 'test': len(train_input_sentence)}\n",
        "print(f'dataset_sizes = {dataset_sizes_sentences}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAwNSS5M0qhT"
      },
      "source": [
        "### Definición de la RNN\n",
        "\n",
        "Además, podemos utilizar la misma arquitectura, solo necesitamos una capa oculta más grande porque las secuencias serán más largas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dz9h0YlQ4fhG"
      },
      "outputs": [],
      "source": [
        "# RNN más grande\n",
        "sentences_rnn = RNN(num_letters, 256, num_letters).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDqLqS1k0sUD"
      },
      "source": [
        "### Entrenamiento\n",
        "\n",
        "Ahora, entrenaremos durante otros 10 ciclos. Entrenar modelos recurrentes normalmente lleva mucho tiempo. El aumento de ciclos debería mejorar el rendimiento del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZhztvmL1KLZ"
      },
      "outputs": [],
      "source": [
        "# Este proceso debería parecerse mucho a tareas anteriores\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10 # Solo completamos 10 ciclos para ahorrar tiempo, pero puede completar más si quiere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doZ1fwqN4r-m",
        "outputId": "0efd1e16-16c5-411a-d82f-766c8e9fac85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/10\n",
            "----------\n",
            "train Loss: 0.1289\n",
            "val   Loss: 0.4593\n",
            "test  Loss: 0.4616\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "train Loss: 0.1097\n",
            "val   Loss: 0.4392\n",
            "test  Loss: 0.4405\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "train Loss: 0.1039\n",
            "val   Loss: 0.4243\n",
            "test  Loss: 0.4252\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "train Loss: 0.0999\n",
            "val   Loss: 0.4111\n",
            "test  Loss: 0.4112\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "train Loss: 0.0963\n",
            "val   Loss: 0.4033\n",
            "test  Loss: 0.4029\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "train Loss: 0.0931\n",
            "val   Loss: 0.3987\n",
            "test  Loss: 0.3979\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "train Loss: 0.0902\n",
            "val   Loss: 0.3943\n",
            "test  Loss: 0.3936\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "train Loss: 0.0873\n",
            "val   Loss: 0.3934\n",
            "test  Loss: 0.3928\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "train Loss: 0.0846\n",
            "val   Loss: 0.3912\n",
            "test  Loss: 0.3906\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "train Loss: 0.0820\n",
            "val   Loss: 0.3901\n",
            "test  Loss: 0.3899\n",
            "\n",
            "Training complete in 5m 51s\n",
            "Best val Loss: 0.082030 at epoch 9\n"
          ]
        }
      ],
      "source": [
        "# Pérdida y optimizador\n",
        "criterion = nn.CrossEntropyLoss() # Utilizar CrossEntropyLoss para la clasificación\n",
        "optimizer = torch.optim.Adam(sentences_rnn.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "\n",
        "# Entrenar el modelo. También almacenaremos los resultados del entrenamiento para poder visualizarlos\n",
        "sentences_rnn, training_curves_sentences = train_rnn(sentences_rnn, dataloaders_sentences, dataset_sizes, \n",
        "                                     criterion, optimizer, scheduler, num_epochs=num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYQTMMVX03v7"
      },
      "source": [
        "### Visualizar los resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "w-FABBjpC6JV",
        "outputId": "909d99cb-4ab8-4238-b963-8ef9a239e324"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_training_curves(training_curves_sentences, phases=['train', 'val', 'test'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DU6G9V4505v-"
      },
      "source": [
        "### Generar frases\n",
        "\n",
        "Podemos utilizar la misma función en el nuevo modelo para generar frases. Pruebe algunos de estos ejemplos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82X4oC3S7aHo",
        "outputId": "4eaddfec-bde0-4337-f1c5-6c8b8a206294"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I ate a what to way\n"
          ]
        }
      ],
      "source": [
        "print(predict(sentences_rnn, \"I ate a \"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIyXR6cMQzDP",
        "outputId": "17ab0eb7-1843-44fc-f161-7d4c33420280"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is a but to the pooner what tom wint to to sou\n"
          ]
        }
      ],
      "source": [
        "print(predict(sentences_rnn, \"What is\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifIHKZgxQ3mt",
        "outputId": "71be248d-060f-4b55-ddf7-208fc0471bda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "My name is a bot me to the poon\n"
          ]
        }
      ],
      "source": [
        "print(predict(sentences_rnn, \"My name is\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0QadAaXQ8C_",
        "outputId": "f0b9ecb9-64aa-43da-87d2-e2e05e3bc8fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hat i don't loke to the poon\n"
          ]
        }
      ],
      "source": [
        "print(predict(sentences_rnn, \"Ha\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwGefkBg4NLx",
        "outputId": "8c6263ad-ffcd-43a5-ab11-1f50dd68bc7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ao look do bust of the pack\n"
          ]
        }
      ],
      "source": [
        "print(predict(sentences_rnn, \"A\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mr3BkX8z1WNl"
      },
      "source": [
        "## Conclusión\n",
        "\n",
        "En este cuaderno, hemos aprendido a aplicar las redes neuronales profundas para resolver distintas tareas secuenciales basadas en la predicción. Los resultados obtenidos al completar frases demuestran que la red no puede producir resultados especialmente inteligentes. En futuros cuadernos, examinaremos formas de mejorar los modelos y resolver tareas más complejas, como la traducción automática. \n",
        "\n",
        "Entre los métodos que permiten mejorar el rendimiento están el uso de conjuntos de datos más grandes, el aumento del tiempo de entrenamiento, las incrustaciones más inteligentes a nivel de palabra (word2vec) y la aplicación de redes neuronales profundas más sofisticadas (LSTM y transformadores)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13 (main, Aug 25 2022, 23:26:10) \n[GCC 11.2.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "1b4dcc508076e9239aa6a2b739d41c8a505780e648b690f375f7d262ba9ac310"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
